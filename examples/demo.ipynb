{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e31c5",
   "metadata": {},
   "source": [
    "# RAJNI-ViT Demo\n",
    "\n",
    "**RAJNI: Relative Adaptive Jacobian-based Neuronal Importance**\n",
    "\n",
    "This notebook demonstrates how to use RAJNI for efficient Vision Transformer inference through adaptive token pruning. We'll cover:\n",
    "\n",
    "1. Loading a pretrained ViT and wrapping it with RAJNI\n",
    "2. Running inference with different pruning intensities (gamma values)\n",
    "3. Visualizing which tokens get pruned at each layer\n",
    "4. Measuring FLOPs reduction compared to the baseline\n",
    "\n",
    "---\n",
    "\n",
    "## Key Idea\n",
    "\n",
    "RAJNI approximates the gradient of the CLS token with respect to each patch token using attention weights and value norms:\n",
    "\n",
    "$$\\text{importance}_j \\approx \\sum_h |A_{0j}^h| \\cdot \\|V_j^h\\|$$\n",
    "\n",
    "This captures how much each patch contributes to the final classification—no backpropagation needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install dependencies if needed\n",
    "# !pip install torch torchvision timm matplotlib numpy\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')  # Add parent directory for imports\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00419009",
   "metadata": {},
   "source": [
    "## 1. Load a Pretrained Vision Transformer\n",
    "\n",
    "We'll use `timm` to load a ViT-Small model. RAJNI works with any ViT architecture from timm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca401335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a ViT-Small model (you can swap this for vit_base_patch16_224, deit_small_patch16_224, etc.)\n",
    "base_model = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
    "base_model.eval()\n",
    "base_model.to(device)\n",
    "\n",
    "print(f\"Model: {base_model.__class__.__name__}\")\n",
    "print(f\"Embedding dim: {base_model.embed_dim}\")\n",
    "print(f\"Number of blocks: {len(base_model.blocks)}\")\n",
    "print(f\"Number of heads: {base_model.blocks[0].attn.num_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac89d89d",
   "metadata": {},
   "source": [
    "## 2. Wrap with RAJNI\n",
    "\n",
    "The `AdaptiveJacobianPrunedViT` wrapper adds token pruning to the forward pass.\n",
    "\n",
    "**Key parameters:**\n",
    "- `gamma`: Pruning intensity (higher = more aggressive). Start with 0.01\n",
    "- `min_tokens`: Floor on token count to prevent over-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rajni import AdaptiveJacobianPrunedViT\n",
    "\n",
    "# Wrap the base model with RAJNI\n",
    "gamma = 0.02  # Moderate pruning intensity\n",
    "model = AdaptiveJacobianPrunedViT(base_model, gamma=gamma, min_tokens=16)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bcd581",
   "metadata": {},
   "source": [
    "## 3. Run Inference and Inspect Pruning\n",
    "\n",
    "Let's create a random image and see how tokens get pruned across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy image (in practice, you'd load a real image)\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# Run forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(dummy_input)\n",
    "\n",
    "# Get pruning statistics\n",
    "stats = model.get_last_stats()\n",
    "token_counts = stats[\"token_counts\"]\n",
    "\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"\\nToken counts per layer:\")\n",
    "for i, count in enumerate(token_counts):\n",
    "    bar = \"█\" * (count // 5)\n",
    "    print(f\"  Layer {i+1:2d}: {count:3d} tokens {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c0b29",
   "metadata": {},
   "source": [
    "## 4. Compare Different Gamma Values\n",
    "\n",
    "Let's see how pruning intensity affects token retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different gamma values\n",
    "gamma_values = [0.0, 0.01, 0.02, 0.05, 0.1]\n",
    "results = {}\n",
    "\n",
    "for g in gamma_values:\n",
    "    test_model = AdaptiveJacobianPrunedViT(base_model, gamma=g, min_tokens=16)\n",
    "    test_model.to(device).eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = test_model(dummy_input)\n",
    "    \n",
    "    stats = test_model.get_last_stats()\n",
    "    results[g] = stats[\"token_counts\"]\n",
    "\n",
    "# Plot token retention curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "for g, counts in results.items():\n",
    "    plt.plot(range(1, len(counts) + 1), counts, marker='o', label=f'γ = {g}')\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Token Count')\n",
    "plt.title('Token Retention Across Layers (Different γ Values)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c8289",
   "metadata": {},
   "source": [
    "## 5. Compute FLOPs Reduction\n",
    "\n",
    "RAJNI reduces compute by processing fewer tokens in later layers. Let's quantify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.flops import flops_reduction\n",
    "\n",
    "# Compute FLOPs for each gamma setting\n",
    "print(\"FLOPs Analysis:\\n\" + \"=\" * 50)\n",
    "print(f\"{'Gamma':<10} {'Baseline':>12} {'RAJNI':>12} {'Reduction':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for g in gamma_values:\n",
    "    test_model = AdaptiveJacobianPrunedViT(base_model, gamma=g, min_tokens=16)\n",
    "    test_model.to(device).eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = test_model(dummy_input)\n",
    "    \n",
    "    stats = test_model.get_last_stats()\n",
    "    flops_info = flops_reduction(base_model, stats)\n",
    "    \n",
    "    print(f\"γ = {g:<6.3f} {flops_info['baseline_GFLOPs']:>10.2f} G  \"\n",
    "          f\"{flops_info['rajni_GFLOPs']:>10.2f} G  \"\n",
    "          f\"{flops_info['reduction_%']:>10.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83731a34",
   "metadata": {},
   "source": [
    "## 6. Visualize Pruned Patches\n",
    "\n",
    "Load a real image and see which patches RAJNI considers important vs. unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "# Download a sample image (cat from ImageNet)\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
    "try:\n",
    "    with urllib.request.urlopen(url, timeout=5) as response:\n",
    "        img_data = response.read()\n",
    "    pil_img = Image.open(io.BytesIO(img_data)).convert('RGB')\n",
    "    print(\"Downloaded sample image\")\n",
    "except:\n",
    "    # Fallback: create a synthetic image with a clear subject\n",
    "    print(\"Using synthetic image (no internet)\")\n",
    "    pil_img = Image.new('RGB', (224, 224), color='white')\n",
    "    # Draw a simple circle in the center\n",
    "    import numpy as np\n",
    "    arr = np.ones((224, 224, 3), dtype=np.uint8) * 255\n",
    "    y, x = np.ogrid[:224, :224]\n",
    "    mask = (x - 112)**2 + (y - 112)**2 < 50**2\n",
    "    arr[mask] = [200, 100, 50]  # Orange circle\n",
    "    pil_img = Image.fromarray(arr)\n",
    "\n",
    "# Preprocess for ViT\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "img_tensor = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "plt.imshow(pil_img)\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74869ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAJNI on the real image and visualize pruning\n",
    "from rajni.utils import denormalize_image\n",
    "\n",
    "# Use moderate pruning for clear visualization\n",
    "vis_model = AdaptiveJacobianPrunedViT(base_model, gamma=0.05, min_tokens=16)\n",
    "vis_model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = vis_model(img_tensor)\n",
    "\n",
    "stats = vis_model.get_last_stats()\n",
    "kept_indices = stats[\"kept_indices\"]\n",
    "\n",
    "# Denormalize for display\n",
    "img_np = img_tensor[0].cpu().permute(1, 2, 0).numpy()\n",
    "img_display = denormalize_image(img_np)\n",
    "\n",
    "# Visualize pruning across selected layers\n",
    "patch_size = 16\n",
    "patches_per_row = 224 // patch_size  # 14\n",
    "total_patches = patches_per_row ** 2  # 196\n",
    "\n",
    "# Track which patches survive\n",
    "alive_patches = set(range(total_patches))\n",
    "\n",
    "layers_to_show = [0, 3, 6, 9, 11] if len(kept_indices) >= 12 else list(range(min(5, len(kept_indices))))\n",
    "\n",
    "fig, axes = plt.subplots(1, len(layers_to_show) + 1, figsize=(3 * (len(layers_to_show) + 1), 3))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(img_display)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show pruning at each layer\n",
    "alive_ids = list(range(total_patches))\n",
    "\n",
    "for ax_idx, layer_idx in enumerate(layers_to_show):\n",
    "    if layer_idx >= len(kept_indices):\n",
    "        break\n",
    "        \n",
    "    keep_idx = kept_indices[layer_idx]\n",
    "    \n",
    "    # Remove CLS (index 0) and adjust\n",
    "    patch_keep = keep_idx[keep_idx != 0].cpu().numpy() - 1\n",
    "    \n",
    "    # Only keep valid indices\n",
    "    patch_keep = patch_keep[(patch_keep >= 0) & (patch_keep < len(alive_ids))]\n",
    "    \n",
    "    # Update alive patches\n",
    "    alive_ids = [alive_ids[i] for i in patch_keep]\n",
    "    pruned = set(range(total_patches)) - set(alive_ids)\n",
    "    \n",
    "    # Draw\n",
    "    ax = axes[ax_idx + 1]\n",
    "    ax.imshow(img_display)\n",
    "    ax.set_title(f\"Layer {layer_idx + 1}\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Highlight pruned patches in blue\n",
    "    for p in pruned:\n",
    "        r, c = p // patches_per_row, p % patches_per_row\n",
    "        rect = plt.Rectangle((c * patch_size, r * patch_size), patch_size, patch_size,\n",
    "                            linewidth=0, facecolor='blue', alpha=0.4)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "plt.suptitle(f\"RAJNI Pruning Visualization (γ = 0.05)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal token count: {len(alive_ids)} / {total_patches} patches \"\n",
    "      f\"({100 * len(alive_ids) / total_patches:.1f}% retained)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5502b80",
   "metadata": {},
   "source": [
    "## 7. Understanding the Algorithm\n",
    "\n",
    "RAJNI's pruning decision at each layer uses three quantities:\n",
    "\n",
    "1. **CLS Sensitivity (ρ)**: How strongly the CLS token attends to patches\n",
    "2. **Mass (η)**: Total importance mass in the current layer  \n",
    "3. **Keep Ratio**: Computed as $(ρ \\cdot η)^{-γ}$, clamped to [0, 1]\n",
    "\n",
    "Higher gamma → more aggressive pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b57e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the pruning functions directly\n",
    "from rajni.pruning import compute_cls_sensitivity, compute_jacobian_importance, compute_keep_ratio\n",
    "\n",
    "# These are the building blocks of RAJNI\n",
    "print(\"Core Pruning Functions:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "1. compute_cls_sensitivity(attention, values)\n",
    "   → Measures how much CLS token \"sees\" each patch\n",
    "   \n",
    "2. compute_jacobian_importance(attention, values, num_patches)\n",
    "   → Approximates ∂CLS/∂patch using attention × value norms\n",
    "   \n",
    "3. compute_keep_ratio(rho, mass, prev_mass, gamma)\n",
    "   → Adaptive budget: (rho × mass/prev_mass)^(-gamma)\n",
    "   \n",
    "4. select_tokens(importance, num_keep, device)\n",
    "   → Top-k selection with CLS always preserved\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2670ceaa",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "- **Benchmark on ImageNet**: Use `examples/run_imagenet.py` \n",
    "- **Hyperparameter sweep**: Run `scripts/sweep_gamma.sh`\n",
    "- **Compare models**: Try different ViT variants (tiny, small, base)\n",
    "\n",
    "For questions or issues, check the README or open a GitHub issue!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
